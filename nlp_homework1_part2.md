作业1（第2部分）：
自然语言处理
Uzair Ahmad
文本分类作业：电影评论情感分析

目标：
本作业的目标是在NLTK电影评论数据集上实现并比较三种文本分类算法——朴素贝叶斯、逻辑回归和多层感知机(MLP)。你将探索使用原始词频(TF)和词频-逆文档频率(TF-IDF)作为特征表示的影响。
___
任务：
1. 数据准备(3分)：
加载NLTK电影评论数据集。
以下是获取IMDb电影评论数据集的方法：

下载数据集：
你可以从NLTK库下载数据集。NLTK提供了一个方便的接口来访问这个数据集。

访问数据集：
下载数据集后，你可以使用以下代码访问电影评论及其对应的标签：
```python
import nltk
# 下载IMDb电影评论数据集
nltk.download('movie_reviews')
from nltk.corpus import movie_reviews

# 访问电影评论和标签
documents = [(list(movie_reviews.words(fileid)), category)
for category in movie_reviews.categories()
for fileid in movie_reviews.fileids(category)]

# 打乱文档以确保正面和负面评论的均衡分布
import random
random.shuffle(documents)

# 打印第一条评论及其标签
print("样本评论:", documents[0][0][:10]) # 为简洁起见只显示前10个词
print("标签:", documents[0][1])
```

探索数据集：
查看数据集的结构和样本评论以了解其特征。
这个IMDb电影评论数据集适合情感分析任务，它在正面和负面评论之间提供了良好的平衡。

使用分词(使用nltk punkt分词器)、词干提取/词形还原和去除停用词对数据集进行预处理。
___
2. 覆盖率分析见解(2分)：
- 进行覆盖率分析以确定预处理步骤覆盖的独特词的百分比。
- 使用y轴表示覆盖率百分比、x轴表示考虑的词数(tokens)来可视化覆盖率分析。使用折线图以清晰显示。
- 讨论从覆盖率分析中获得的见解。考虑以下问题：
  - 覆盖率如何随考虑的词数变化？
  - 在什么点覆盖率趋于稳定？
  - 随着词数增加，覆盖率的收益是否递减？

词汇选择的理由：
讨论选择特定词汇量进行建模的理由。考虑以下因素：
- 更大词汇量(更多词)与计算效率之间的权衡。
- 罕见词或非常常见词对模型泛化能力的影响。
- 平衡信息量和模型复杂度的需要。
- 就词汇量而言对所选算法(朴素贝叶斯、逻辑回归、MLP)的具体考虑。
___
3. 算法实现(6分)：
a. 朴素贝叶斯：
 - 实现多项式朴素贝叶斯分类器。
 - 使用TF和TF-IDF作为特征表示来训练和测试模型。

b. 逻辑回归：
 - 实现逻辑回归分类器。
 - 使用TF和TF-IDF进行训练和测试。

c. 多层感知机(MLP)：
 - 实现基于MLP的分类器。
 - 探索不同的架构(层数、每层神经元数)。
 - 使用TF和TF-IDF进行训练和测试。
___
4. 训练和评估(4分)：
a. 在训练集上训练每个算法。
b. 使用准确率、TPR、FPR作为主要指标在测试集上评估每个算法的性能。
c. 比较使用TF和TF-IDF对每个算法性能的影响。
___
5. 可视化和分析(2分)：
a. 使用适当的图表(如条形图)可视化每个算法的性能指标(如准确率)。
b. 讨论观察到的性能趋势或差异。
___
6. 讨论(3分)：
a. 比较和分析三种算法获得的结果。
b. 讨论使用TF与TF-IDF对分类性能的影响。
c. 提供每种算法在情感分析环境下的优势和局限性的见解。
___
提交指南：
- 提交每个算法实现的Python代码。
- 包含一个README文件，其中包含如何运行代码的说明和任何依赖项。
- 提交一份包含详细解释、可视化和比较分析的报告文档(PDF)。
