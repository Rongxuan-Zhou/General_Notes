`nltk`（**Natural Language Toolkit**）是 Python 中一个功能强大的自然语言处理（NLP）工具包，提供了丰富的模块和功能，用于文本处理、分词、标注、解析、语料库操作等任务。以下是 `nltk` 的主要功能和用法：

---

## **1. 安装 NLTK**

### **安装工具包**
在终端中运行以下命令安装 `nltk`：
```bash
pip install nltk
```

### **下载语料库和模型**
`nltk` 提供了许多内置的语料库和模型。需要先下载这些资源：
```python
import nltk
nltk.download('all')  # 下载所有资源（耗时较长）
# 或下载特定资源
nltk.download('punkt')  # 用于分词
nltk.download('stopwords')  # 用于停用词
nltk.download('wordnet')  # 用于词汇和同义词处理
```

---

## **2. 基本用法**

### **2.1 分词（Tokenization）**
将文本分割成单词或句子，是 NLP 的基础操作之一。

#### **句子分割**
```python
from nltk.tokenize import sent_tokenize
text = "Natural Language Toolkit is amazing. It makes text processing easy!"
sentences = sent_tokenize(text)
print(sentences)
# 输出：['Natural Language Toolkit is amazing.', 'It makes text processing easy!']
```

#### **单词分割**
```python
from nltk.tokenize import word_tokenize
words = word_tokenize(text)
print(words)
# 输出：['Natural', 'Language', 'Toolkit', 'is', 'amazing', '.', 'It', 'makes', 'text', 'processing', 'easy', '!']
```

---

### **2.2 停用词（Stopwords）**
停用词是 NLP 中常见的无意义词（如 "is"、"the" 等）。`nltk` 提供了内置的停用词列表。

```python
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
words = word_tokenize("This is a simple example showing stop word removal.")
filtered_words = [word for word in words if word.lower() not in stop_words]
print(filtered_words)
# 输出：['This', 'simple', 'example', 'showing', 'stop', 'word', 'removal', '.']
```

---

### **2.3 词性标注（POS Tagging）**
为每个单词标注词性（如名词、动词等）。

```python
from nltk import pos_tag
from nltk.tokenize import word_tokenize

text = "NLTK is a leading platform for building Python programs to work with human language data."
words = word_tokenize(text)
tags = pos_tag(words)
print(tags)
# 输出：[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ...]
```

- 常见 POS 标签含义：
  - `NN`：名词
  - `VB`：动词
  - `JJ`：形容词
  - `RB`：副词

---

### **2.4 词干提取（Stemming）**
将单词还原为其词干形式。

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
words = ["running", "jumps", "easily", "faster"]
stems = [stemmer.stem(word) for word in words]
print(stems)
# 输出：['run', 'jump', 'easili', 'faster']
```

---

### **2.5 词形还原（Lemmatization）**
与词干提取类似，但结果更精确，考虑了语法和上下文。

```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
words = ["running", "jumps", "easily", "faster"]
lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]  # pos 参数指定词性
print(lemmas)
# 输出：['run', 'jump', 'easily', 'fast']
```

---

### **2.6 语料库操作**
`nltk` 提供了丰富的内置语料库，可以直接加载使用。

#### **停用词**
```python
from nltk.corpus import stopwords
print(stopwords.words('english')[:10])
# 输出：['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']
```

#### **单词同义词**
使用 WordNet 查找单词的同义词和反义词。
```python
from nltk.corpus import wordnet

synonyms = wordnet.synsets("good")
print([syn.name() for syn in synonyms])
# 输出：['good.n.01', 'good.n.02', 'good.n.03', ...]

# 获取单词定义
print(synonyms[0].definition())
# 输出：'Benefit'

# 获取同义词和反义词
print(synonyms[0].lemmas()[0].name())  # 同义词
print(synonyms[0].lemmas()[0].antonyms())  # 反义词
```

---

### **2.7 N-Grams**
生成 N-Grams（N 个连续单词的组合）。

```python
from nltk import ngrams

text = "I love natural language processing"
tokens = word_tokenize(text)
bigrams = list(ngrams(tokens, 2))  # 生成二元组
print(bigrams)
# 输出：[('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing')]
```

---

### **2.8 简单文本分类**
使用 `nltk` 的朴素贝叶斯分类器进行简单文本分类。

#### 示例：
```python
from nltk.classify import NaiveBayesClassifier
from nltk.classify.util import accuracy

# 数据集
train_data = [({'word': 'I love Python'}, 'Positive'),
              ({'word': 'Python is amazing'}, 'Positive'),
              ({'word': 'I hate bugs'}, 'Negative'),
              ({'word': 'Debugging is frustrating'}, 'Negative')]

# 训练分类器
classifier = NaiveBayesClassifier.train(train_data)

# 测试
test_data = {'word': 'I love debugging'}
print(classifier.classify(test_data))  # 输出：Positive
```

---

### **3. 高级功能**

#### **依存解析与句法分析**
可以使用 `nltk` 的 `CFG`（上下文无关文法）模块构建语法规则并解析句子。

#### **命名实体识别（NER）**
识别文本中的实体（如人名、地名等）：
```python
from nltk import ne_chunk, pos_tag, word_tokenize

text = "Barack Obama was the 44th President of the United States."
chunks = ne_chunk(pos_tag(word_tokenize(text)))
print(chunks)
```

---

### **4. 常见问题**
1. **NLTK 语料库未下载**
   - 错误：`LookupError: Resource not found`
   - 解决：运行 `nltk.download('resource_name')` 下载缺失的资源。

2. **性能问题**
   - `nltk` 适合学习和简单 NLP 项目，但处理大规模数据时性能较慢。可以结合其他工具（如 `spaCy` 和 `Hugging Face`）。
